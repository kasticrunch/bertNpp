# -*- coding: utf-8 -*-
"""fineTuningBertWithMorp_huggingFace.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cb8rL6OiJorVL0ArWUb_FzLtgICdw7vd
"""

! pip install git+https://github.com/huggingface/transformers.git
! pip install transformers datasets
! pip install pandas
! pip install sklearn
! pip install transformers
! pip install --user urllib3==1.25.10
! pip install folium==0.2.1
#! pip install comet_ml
#! pip install comet_ml --upgrade

#import comet_ml
import pandas as pd
import numpy as np

from morpMetrics import Metrics
from inputData import Data
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset, dataset_dict, DatasetDict, Dataset, load_metric
from sklearn.model_selection import train_test_split
from pprint import pprint as pp
from sklearn.metrics import precision_recall_curve, roc_curve


#comet_ml.init()

#experiment = comet_ml.Experiment(
#    project_name="confusion-matrix", 
#)

csvInput = 'noRefuelingOutagesUnderSampled.csv'
df = Data.getData(csvInput)
maxTextLength = df.text.map(len).max()

N_CLASSES = 3
N_EPOCHS = 100


dataset = Dataset.from_pandas(df)
train_test = dataset.train_test_split(test_size=0.30,shuffle=True) 
test_valid = train_test['test'].train_test_split(test_size=0.70,shuffle=True)

# datasets object containing individual 'train', 'test' and 'valid' Datasets
mainDataset = DatasetDict({
    'train': train_test['train'],
    'test': test_valid['test'],
    'valid': test_valid['train']})

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
"More info at https://huggingface.co/bert-base-uncased"
def tokenize_function(example):
    """When provided an example, will return a tokenized vector of length `maxTextLength`.
    pad vector to `maxTextLength` if 'text' is < maxTextLength.
    truncate to `maxTextLength`
    """
    return tokenizer(example['text'],max_length=maxTextLength,padding="max_length", truncation=True, add_special_tokens=True)

# datasets object containing individual 'train', 'test' and 'valid' Datasets with 'attention_mask','input_ids' and 'token_type_ids' features
tokenized_datasets = mainDataset.map(tokenize_function,batched=True).remove_columns(['text','__index_level_0__'])
################# Pick up here
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42)
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42)
full_train_dataset = tokenized_datasets["train"]
full_eval_dataset = tokenized_datasets["test"]

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)
training_args = TrainingArguments("test_trainer",
                                  evaluation_strategy="epoch",
                                  num_train_epochs=N_EPOCHS,
                                  per_device_train_batch_size=8,
                                  per_device_eval_batch_size=32,
                                  logging_dir="bert_results/logs",
                                  logging_steps=10,
                                  )

trainer = Trainer(model=model,
                  args=training_args,
                  train_dataset=full_train_dataset,
                  eval_dataset=full_eval_dataset,
                  Metrics.computeMetrics=Metrics.computeMetrics,
                  )

output = trainer.train()
print(output)

trainer.evaluate()

prediction = trainer.predict(small_eval_dataset)
print(prediction)

# test_predict = trainer.predict(testDataset['train'])
# print(test_predict)

for i,k in zip(prediction[0],prediction[1]):
  print(i,k)

len(small_eval_dataset['labels'])

experiment.end()

